import OpenAI from 'openai';
import fs from 'fs/promises';
import path from 'path';
import logger from '../utils/logger';
import apiLogger from '../utils/apiLogger';

interface TestScenario {
  summary: string;
  actions: string[];
}

interface GeneratedTest {
  code: string;
  annotations: string[];
  timestamp: Date;
}

class TestGenerationService {
  private openai: OpenAI;
  private logFilePath: string;

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY;

    if (!apiKey || apiKey === 'your_openai_api_key_here') {
      logger.warn('OpenAI API key not configured. Test generation will be disabled.');
      this.openai = null as any;
    } else {
      this.openai = new OpenAI({
        apiKey: apiKey
      });
    }

    this.logFilePath = path.join(process.cwd(), 'logs_code.txt');
    this.initializeLogFile();
  }

  private async initializeLogFile(): Promise<void> {
    try {
      await fs.access(this.logFilePath);
    } catch {
      await fs.writeFile(this.logFilePath, '# Test Generation Logs\n# Generated by AI Testing Agent\n\n', 'utf8');
      logger.info(`Initialized test generation log file at: ${this.logFilePath}`);
    }
  }

  /**
   * Generate test code based on a scenario description
   * @param scenario - The test scenario with summary and actions
   * @returns Generated test code with annotations
   */
  async generateTest(scenario: TestScenario): Promise<GeneratedTest> {
    if (!this.openai) {
      throw new Error('OpenAI API key not configured. Please add OPENAI_API_KEY to your .env file.');
    }

    try {
      logger.info('Generating test for scenario:', { summary: scenario.summary });

      // Prepare the prompt for GPT-4o-mini
      const prompt = this.buildPrompt(scenario);

      // Start timer for duration tracking
      const startTime = Date.now();

      // Call OpenAI API with GPT-5 nano - the cheapest model
      // GPT-5 nano: $0.05/1M input ($0.00005/1K), $0.40/1M output ($0.0004/1K)
      // 10x cheaper than gpt-3.5-turbo!

      const model = 'gpt-5-nano';
      const baseParams: any = {
        model: model,  // Cheapest model - perfect for test generation
        messages: [
          {
            role: 'system',
            content: `You are an expert test automation engineer. Generate clean, well-annotated test code based on the provided scenario.
            Rules:
            1. Add clear comments explaining what each section does
            2. Use modern JavaScript/TypeScript syntax
            3. Include proper error handling
            4. Make the code production-ready
            5. Add annotations for each major step`
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 2000
      };

      // Only add temperature if not GPT-5-nano (which may not support it)
      if (!model.includes('gpt-5-nano')) {
        baseParams.temperature = 0.3;
      }

      // Log the API request
      const requestId = await apiLogger.logRequest(
        'generate_test',
        model,
        { ...scenario, ...baseParams },
        startTime
      );

      let completion;
      let actualModel = model;
      try {
        completion = await this.openai.chat.completions.create(baseParams);
      } catch (error: any) {
        // Fallback to gpt-3.5-turbo if GPT-5-nano fails
        if (error.code === 'model_not_found' || error.message?.includes('model')) {
          logger.warn('GPT-5-nano not available, falling back to gpt-3.5-turbo');
          actualModel = 'gpt-3.5-turbo';
          baseParams.model = actualModel;
          baseParams.temperature = 0.3;
          completion = await this.openai.chat.completions.create(baseParams);
        } else {
          // Log error
          await apiLogger.logResponse(requestId, 'generate_test', model, scenario, null, startTime, error);
          throw error;
        }
      }

      const generatedCode = completion.choices[0]?.message?.content || '';

      // Log successful response
      await apiLogger.logResponse(
        requestId,
        'generate_test',
        actualModel,
        scenario,
        completion,
        startTime
      );

      // Extract annotations from the code
      const annotations = this.extractAnnotations(generatedCode);

      // Get token usage info
      const tokenInfo = completion.usage ? {
        input: completion.usage.prompt_tokens || 0,
        output: completion.usage.completion_tokens || 0,
        total: completion.usage.total_tokens || 0
      } : undefined;

      // Log the generated test with complete input and output
      const generationId = await this.logGeneratedTest(
        scenario,
        generatedCode,
        annotations,
        prompt,
        actualModel,
        tokenInfo
      );

      return {
        code: generatedCode,
        annotations: annotations,
        timestamp: new Date()
      };
    } catch (error: any) {
      logger.error('Error generating test:', error);
      throw new Error(`Test generation failed: ${error.message}`);
    }
  }

  /**
   * Build the prompt for test generation
   */
  private buildPrompt(scenario: TestScenario): string {
    const actionsFormatted = scenario.actions
      .map((action, index) => `${index + 1}. ${action}`)
      .join('\n');

    return `Generate a test for the following scenario:

Summary: ${scenario.summary}

Actions to perform:
${actionsFormatted}

Please generate:
1. Complete test code that performs all the actions
2. Add detailed comments and annotations
3. Include proper setup and teardown
4. Add error handling for each critical step
5. Make it compatible with common testing frameworks (Jest/Mocha/Playwright)`;
  }

  /**
   * Extract annotations/comments from generated code
   */
  private extractAnnotations(code: string): string[] {
    const annotations: string[] = [];
    const lines = code.split('\n');

    for (const line of lines) {
      // Extract single-line comments
      if (line.trim().startsWith('//')) {
        annotations.push(line.trim().substring(2).trim());
      }
      // Extract multi-line comment content
      if (line.includes('/*')) {
        const commentMatch = line.match(/\/\*(.+?)\*\//);
        if (commentMatch) {
          annotations.push(commentMatch[1].trim());
        }
      }
    }

    return annotations;
  }

  /**
   * Log generated test to logs_code.txt file with complete input and output
   */
  private async logGeneratedTest(
    scenario: TestScenario,
    code: string,
    annotations: string[],
    prompt?: string,
    model?: string,
    tokenInfo?: any
  ): Promise<string> {
    const timestamp = new Date().toISOString();
    const separator = '='.repeat(80);
    const subSeparator = '-'.repeat(40);

    // Generate unique ID for this generation
    const generationId = `gen_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

    // Create individual log directory
    const logsDir = path.join(process.cwd(), 'generation_logs');
    await fs.mkdir(logsDir, { recursive: true });

    // Create individual generation log file
    const individualLogPath = path.join(logsDir, `${generationId}.json`);

    const logData = {
      id: generationId,
      timestamp,
      model: model || 'Unknown',
      scenario: {
        summary: scenario.summary,
        actions: scenario.actions
      },
      input: {
        prompt: prompt || 'Prompt not captured'
      },
      output: {
        code,
        annotations,
        tokenInfo: tokenInfo || null,
        estimatedCost: tokenInfo ? ((tokenInfo.input * 0.00005) + (tokenInfo.output * 0.0004)).toFixed(6) : null
      }
    };

    // Save individual log as JSON
    await fs.writeFile(individualLogPath, JSON.stringify(logData, null, 2), 'utf8');

    // Also update the index file
    const indexPath = path.join(logsDir, 'index.json');
    let index = [];
    try {
      const indexData = await fs.readFile(indexPath, 'utf8');
      index = JSON.parse(indexData);
    } catch {
      // Index doesn't exist yet
    }

    index.push({
      id: generationId,
      timestamp,
      summary: scenario.summary,
      model: model || 'Unknown',
      cost: logData.output.estimatedCost
    });

    // Keep only last 100 entries in index
    if (index.length > 100) {
      index = index.slice(-100);
    }

    await fs.writeFile(indexPath, JSON.stringify(index, null, 2), 'utf8');

    // Also append to the main log file for backward compatibility
    const logEntry = `
${separator}
GENERATION LOG ENTRY (ID: ${generationId})
${separator}
TIMESTAMP: ${timestamp}
MODEL USED: ${model || 'Unknown'}

${subSeparator}
INPUT DETAILS
${subSeparator}
SCENARIO SUMMARY: ${scenario.summary}

TEST ACTIONS:
${scenario.actions.map((action, i) => `  ${i + 1}. ${action}`).join('\n')}

FULL PROMPT SENT TO AI:
${prompt || 'Prompt not captured'}

${subSeparator}
OUTPUT DETAILS
${subSeparator}
ANNOTATIONS EXTRACTED:
${annotations.map((ann, i) => `  ${i + 1}. ${ann}`).join('\n')}

TOKEN USAGE:
${tokenInfo ? `  Input Tokens: ${tokenInfo.input}
  Output Tokens: ${tokenInfo.output}
  Total Tokens: ${tokenInfo.total}
  Estimated Cost: $${((tokenInfo.input * 0.00005) + (tokenInfo.output * 0.0004)).toFixed(6)}` : '  Token information not available'}

COMPLETE GENERATED CODE:
\`\`\`javascript
${code}
\`\`\`

${separator}

`;

    try {
      await fs.appendFile(this.logFilePath, logEntry, 'utf8');
      logger.info(`Test generation logged to ${this.logFilePath} and ${individualLogPath}`);
    } catch (error) {
      logger.error('Failed to log generated test:', error);
    }

    return generationId;
  }

  /**
   * Execute the generated test code
   * @param code - The test code to execute
   * @returns Test execution results
   */
  async executeTest(code: string): Promise<{ success: boolean; output: string; error?: string }> {
    try {
      // For now, we'll save the test to a temporary file and run it
      const testFilePath = path.join(process.cwd(), 'temp', `test_${Date.now()}.js`);

      // Ensure temp directory exists
      await fs.mkdir(path.join(process.cwd(), 'temp'), { recursive: true });

      // Write test file
      await fs.writeFile(testFilePath, code, 'utf8');

      // Here you would execute the test using the appropriate test runner
      // This is a placeholder - actual implementation depends on your test framework
      logger.info(`Test file created at: ${testFilePath}`);

      return {
        success: true,
        output: `Test saved to: ${testFilePath}. Ready for execution.`
      };
    } catch (error: any) {
      logger.error('Test execution failed:', error);
      return {
        success: false,
        output: '',
        error: error.message
      };
    }
  }

  /**
   * Get test generation history from logs
   */
  async getTestHistory(limit: number = 10): Promise<string[]> {
    try {
      const logContent = await fs.readFile(this.logFilePath, 'utf8');
      const entries = logContent.split('='.repeat(80)).filter(entry => entry.trim());
      return entries.slice(-limit);
    } catch (error) {
      logger.error('Failed to read test history:', error);
      return [];
    }
  }
}

export default new TestGenerationService();